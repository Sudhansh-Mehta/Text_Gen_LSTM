# Text Generation using LSTM Networks
Text_Generation with LSTM Networks:
Designed and implemented an architecture that involves a Generator and a Discriminator Model to work in tandem as adversaries trying to outsmart each other but in the process train each other.
For training, the generator model was fed the works of Friedrich Wilhelm Nietzsche, to generate characters to make sentences by statistically sampling the dataset.
Discriminator network employs a Binary Cross Entropy Loss to guide the learning. The architecture deployed reported an accuracy of 86 percent.
Apart from the workâ€™s numerous applications, it gives an insight into the nuances of the cognitive learning process and how learning can be optimized.
